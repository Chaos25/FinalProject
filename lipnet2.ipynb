{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\lib\\site-packages\\scipy\\__init__.py:155: UserWarning: A NumPy version >=1.18.5 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Anaconda\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv3D, MaxPooling3D, TimeDistributed, Bidirectional, LSTM, Dense, Dropout, Flatten\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import tensorflow as tf\n",
    "# Function to load frames\n",
    "def load_frames(folder_path, scale_percent=70):\n",
    "    frames = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".png\"):\n",
    "            frame = cv2.imread(os.path.join(folder_path, filename), cv2.IMREAD_GRAYSCALE)\n",
    "            width = int(frame.shape[1] * scale_percent / 100)\n",
    "            height = int(frame.shape[0] * scale_percent / 100)\n",
    "            dim = (width, height)\n",
    "            resized_frame = cv2.resize(frame, dim, interpolation=cv2.INTER_AREA)\n",
    "            frames.append(resized_frame)\n",
    "    frames = np.array(frames)\n",
    "    frames = np.expand_dims(frames, axis=-1)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load alignments\n",
    "def load_alignments(file_path):\n",
    "    alignments = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            start, end, text = line.strip().split(' ', 2)\n",
    "            alignments.append((int(start), int(end), text))\n",
    "    return alignments\n",
    "\n",
    "# Function to map timestamps to frame indices\n",
    "def map_timestamps_to_frames(alignments, frame_rate):\n",
    "    frame_timestamps = []\n",
    "    for align in alignments:\n",
    "        start, end, _ = align\n",
    "        start_frame = int(start / 1000 * frame_rate)\n",
    "        end_frame = int(end / 1000 * frame_rate)\n",
    "        frame_timestamps.append((start_frame, end_frame))\n",
    "    return frame_timestamps\n",
    "\n",
    "# Function to split data based on frame indices\n",
    "def split_data_by_frame_indices(frames, sequences, frame_timestamps, split_point_frame):\n",
    "    train_frames = []\n",
    "    train_sequences = []\n",
    "    test_frames = []\n",
    "    test_sequences = []\n",
    "    \n",
    "    for i, (start, end) in enumerate(frame_timestamps):\n",
    "        group_frames = frames[start:end]\n",
    "        if len(group_frames) > 0: # Ensure there are frames in the group\n",
    "            if end < split_point_frame:\n",
    "                train_frames.append(group_frames)\n",
    "                train_sequences.append(sequences[i])\n",
    "            elif start >= split_point_frame:\n",
    "                test_frames.append(group_frames)\n",
    "                test_sequences.append(sequences[i])\n",
    "    \n",
    "    return train_frames, test_frames, train_sequences, test_sequences\n",
    "\n",
    "# Function to adjust the length of each group of frames\n",
    "def adjust_frames_length(frames_group, max_length):\n",
    "    if len(frames_group) > max_length:\n",
    "        # Truncate the group to the maximum length\n",
    "        return frames_group[:max_length]\n",
    "    else:\n",
    "        # Pad the group with zeros to the maximum length\n",
    "        padding = np.zeros((max_length - len(frames_group), frames_group[0].shape[0], frames_group[0].shape[1], 1), dtype=np.uint8)\n",
    "        return np.concatenate((frames_group, padding), axis=0)\n",
    "\n",
    "# Function to create the LipNet model\n",
    "def create_lipnet_model(input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv3D(128, (3, 3, 3), activation='relu', padding='same', input_shape=input_shape))\n",
    "    model.add(MaxPooling3D(pool_size=(1, 1, 1)))\n",
    "    model.add(Conv3D(256, (3, 3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling3D(pool_size=(1, 1, 1)))\n",
    "    model.add(Conv3D(75, (3, 3, 3), activation='relu', padding='same'))\n",
    "    model.add(MaxPooling3D(pool_size=(1, 1, 1)))\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='Orthogonal')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Bidirectional(LSTM(128, return_sequences=True, kernel_initializer='Orthogonal')))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load frames and alignments\n",
    "frames_folder = 'D:/Voices-AWS/reading/24fa'\n",
    "alignments_file = 'D:/Voices-AWS/reading/24fa.align'\n",
    "frames = load_frames(frames_folder)\n",
    "alignments = load_alignments(alignments_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate frame rate based on the total number of frames and the video duration\n",
    "video_duration = 101934 # This is the end timestamp of the last alignment in your .align file (in milliseconds)\n",
    "total_frames = 3089\n",
    "frame_rate = total_frames / (video_duration / 1000) # Convert video_duration to seconds\n",
    "\n",
    "frame_timestamps = map_timestamps_to_frames(alignments, frame_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text):\n",
    "    return list(text)\n",
    "\n",
    "sequences = [text_to_sequence(align[2]) for align in alignments]\n",
    "\n",
    "# Split data\n",
    "split_point = 81207 # This should be in milliseconds\n",
    "split_point_frame = int(split_point / 1000 * frame_rate) # Convert split_point to frame index\n",
    "train_frames, test_frames, train_sequences, test_sequences = split_data_by_frame_indices(frames, sequences, frame_timestamps, split_point_frame)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_duration = 101934 # This is the end timestamp of the last alignment in your .align file (in milliseconds)\n",
    "total_frames = frames.shape[0]\n",
    "frame_rate = total_frames / (video_duration / 1000) # Convert video_duration to seconds\n",
    "\n",
    "frame_timestamps = map_timestamps_to_frames(alignments, frame_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "# Determine the maximum number of frames in any group\n",
    "max_frames_per_group = max(max(len(group) for group in train_frames), max(len(group) for group in test_frames))\n",
    "\n",
    "# Adjust the length of each group of frames\n",
    "train_frames_adjusted = [adjust_frames_length(group, max_frames_per_group) for group in train_frames]\n",
    "test_frames_adjusted = [adjust_frames_length(group, max_frames_per_group) for group in test_frames]\n",
    "print(len(train_frames_adjusted))\n",
    "print(len(train_sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "652\n"
     ]
    }
   ],
   "source": [
    "# Extend the mapping to include all unique characters found in the text\n",
    "char_to_int = {char: i for i, char in enumerate('abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789.,!?;:\\'\\\"()[]{}<>_+=@#$%^&*-=|\\\\/ ')} # Include space character\n",
    "char_to_int.update({symbol: i + len(char_to_int) for i, symbol in enumerate(['â†«', 'â†«Fâ†«', '[/]', '‰'])}) # Extended mapping for special symbols\n",
    "# Convert sequences to lists of integers, handling spaces correctly\n",
    "def convert_sequence_to_int(seq):\n",
    "    return [char_to_int[char] for char in seq if char in char_to_int]\n",
    "\n",
    "# Convert sequences to lists of integers using the extended mapping\n",
    "train_sequences_int = [convert_sequence_to_int(seq) for seq in train_sequences]\n",
    "test_sequences_int = [convert_sequence_to_int(seq) for seq in test_sequences]\n",
    "\n",
    "# Pad the sequences\n",
    "max_length = max(max(len(seq) for seq in train_sequences_int), max(len(seq) for seq in test_sequences_int))\n",
    "train_sequences_padded = pad_sequences(train_sequences_int, maxlen=652, padding='post')\n",
    "test_sequences_padded = pad_sequences(test_sequences_int, maxlen=652, padding='post')\n",
    "\n",
    "# # Convert the padded sequences to one-hot encoded vectors\n",
    "train_labels = to_categorical(train_sequences_padded)\n",
    "test_labels = to_categorical(test_sequences_padded)\n",
    "print(len(train_labels[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 27.1 GiB for an array with shape (7172, 756, 1344, 1) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_18332\\316031587.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_frames_single\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_frames_adjusted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Convert to tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtrain_frames_adjusted_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_frames_single\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtrain_labels_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 103\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 27.1 GiB for an array with shape (7172, 756, 1344, 1) and data type float32"
     ]
    }
   ],
   "source": [
    "train_frames_single = np.concatenate(train_frames_adjusted, axis=0)\n",
    "# Convert to tensor\n",
    "train_frames_adjusted_tensor = tf.convert_to_tensor(train_frames_single, dtype=tf.float32)\n",
    "train_labels_tensor = tf.convert_to_tensor(train_labels, dtype=tf.float32)\n",
    "\n",
    "# Ensure the number of samples in train_frames_adjusted_tensor matches the number of samples in train_labels_tensor\n",
    "#assert train_frames_adjusted_tensor.shape[0] == train_labels_tensor.shape[0], \"Data cardinality mismatch\"\n",
    "# Define input shape based on your frames\n",
    "input_shape = (frames.shape[1], frames.shape[2], 1, 1) # Assuming grayscale frames with depth of 1\n",
    "num_classes = len(train_sequences[0]) # Number of characters in the sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and compile the model\n",
    "model = create_lipnet_model(input_shape, num_classes)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(train_frames_adjusted_tensor, train_labels_tensor, epochs=10, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train the model\n",
    "# epochs = 10 # Number of epochs to train\n",
    "# model.fit(train_dataset, epochs=epochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
